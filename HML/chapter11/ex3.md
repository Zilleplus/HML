# Name three advantages of the SELU activation function ofver ReLU
1. Just like the leaky ReLU does it have a gradient greater then zero when x is smaller then 0.
2. It's smoother, so better convergence.
3. It's exponential shape is able to express more complex behavior (vs ReLu that is just a triangle). (not in appendix answer, but it's in the book)
4. SELU can normalize if the conditions are right.
