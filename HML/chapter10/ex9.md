# Can you list the hyper parameters you can tweak in a basic MLP? If the MLP overfits the traning data, how could you tweak these hyperparameters to try to solve the problem.

- Number of hidden layers
- Number of neurons in layers
- Type of activation function

When overfitting you could reduce the number of neurons, or even reduce the number of layers. The flexibility of the model will be lower, so you should see less overfitting.
