\chapter{Monte Carlo Methods}

\section{Exercises}

\subsection{Exercise 5.1 page 94}
The last 2 rows in the rear means you either have 21, or 20, which means the odd's are very good you will win. (hence high value function)

The last row on the left means the dealer has an ace, so it's at an advantage to get a higher score.

The front row's are higher on the upper diagram, as there is a usuable ace. Which means that if you get a bad hit that put's you over 21. It can count as 1.

\subsection{Exercise 5.2 page 94}
As this is Markov process eg. The cards drawn are not exhaustible. The odds of winning on the second time your in the same state is just as good as the first time.

\subsection{Exercise 5.4 page 99}
The "Append G to Returns ($S_{t} , A_{t}$) would be replaced by increasing a count and added it as running average to some table.

\subsection{Exercise 5.5 page 105}
\textbf{question: Consider an MDP with a single Non-terminal state and a single action that transitions back to the nonterminal state with probability $p$ and transitions to the terminal state with probability $p-1$. Let the reward be != on all transitions, and let $\gamma = 1$. Suppose you observe one episode that lasts 10 steps, with a return of 10. What are the first-visit and every visit estimators of the value of the non-terminal state. }

10 Steps means 9 towards the non-terminal, and one towards the terminal. The rewards are all-way's the same so the final cost=10. 

If $\gamma = 1$ then $G=G+\gamma R_{k+1}$ in every iteration. 

In case of all visit the complete horizon counts 10 times in the non-terminal state, as the 10th time we leave the non-terminal state for good and enter the terminal state. $(1+2+3+4+5+6+7+8+9+10)/10 = 55/10 = 5.5$ So the value is 5.

In case of the first-visit, we only count the first visit which has a reward of 1.

\subsection{Exercise 5.6 page 108}
\textbf{question: What is the equation analogous to (5.6) for action values $Q(s,a)$ instead of state values $V(s)$, again given returns generated using b?}

Q(s, a) is similar to V(s), it takes the V(s) given a certain step was taken first.

\begin{equation}
Q(s, a) = \frac{\sum_{t \in J(s,a)} \rho_{t+1:T(t)-1} G_t }{\sum_{t \in J(s,a)} \rho_{t+1:T(t)-1}}
\end{equation}

\subsection{Exercise 5.7 page 108}
\textbf{question: In learning curves such as those shown in Figure 5.3 error generally decreases with training as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened}

If there are but a few samples, the bias will be the dominating error. And it will increase as more and more samples are added. Until there are so many samples, it starts to disappear.

\subsection{Exercise 5.8 page 108}
\textbf{question: The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?}
A first Visit MC has less terms then a every Visit MC. All terms have a positive value, so it would also go to infinite.

\subsection{Exercise 5.11 page 111}
If the target policy is a greedy deterministic policy, and the loop is broken off if $\pi (S_t) \neq A_t$. Then $\pi(A_t|S_t)=1$ by definition.  