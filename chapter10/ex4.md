# Why was the logistic activation function a key ingredient in training the first MLPs?
The step function is flat, so the gradient is zero, and a gradient descent doesn't work. The logistic activation function is nicely flat, and has a nice behaving gradient.
