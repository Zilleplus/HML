#Name three popular activation functions:
1. logistic: between 0-1 smooth upward function. (x=-4; y=0) (x=0;y=0.5) (x=4;y=1)
2. tanh: About same as logistic, but between -1 and 1. 0 is centered wich can speedup convergence.
3. ReLu (very fast convergence) -> step function
